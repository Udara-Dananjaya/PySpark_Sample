{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark** RDD Operations\n",
        "\n",
        "Resilient Distributed Dataset or RDD in a PySpark is a core data structure of PySpark. PySpark RDD's is a low-level object and are highly efficient in performing distributed tasks.\n",
        "\n",
        "\n",
        "PySpark RDD has a set of operations to accomplish any task. These operations are of two types:\n",
        "\n",
        "1. Transformations\n",
        "\n",
        "2. Actions\n",
        "\n",
        "Transformations are a kind of operation that takes an RDD as input and produces another RDD as output. Once a transformation is applied to an RDD, it returns a new RDD, the original RDD remains the same and thus are immutable. After applying the transformation, it creates a Directed Acyclic Graph or DAG for computations and ends after applying any actions on it. This is the reason they are called lazy evaluation processes.\n",
        "\n",
        "Actions are a kind of operation which are applied on an RDD to produce a single value. These methods are applied on a resultant RDD and produces a non-RDD value, thus removing the laziness of the transformation of RDD.\n",
        "\n",
        " Transformations are applied on an RDD to give another RDD. While Actions are performed on an RDD to give a non-RDD value."
      ],
      "metadata": {
        "id": "2K5mcOs6nr-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwsZdouTnrM1",
        "outputId": "4ad13baa-2940-4adc-daa4-597a35d092a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285387 sha256=4f1690795976255675c77b226f4365689f658575cf998dea59074dccf02ca772\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGFcZOeTngfa"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actions in PySpark RDDs**\n",
        "\n",
        "Following are some of the essential PySpark RDD Operations widely used.\n",
        "\n",
        "**1. The .collect() Action**\n",
        "\n",
        "The .collect() action on an RDD returns a list of all the elements of the RDD. It’s a great asset for displaying all the contents of our RDD. Let’s understand this with an example:"
      ],
      "metadata": {
        "id": "-cRfZgZeor5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collect_rdd = sc.parallelize([1,2,3,4,5])\n",
        "print(collect_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dycPl2XypDGQ",
        "outputId": "b4b5a2ed-df7e-467b-d838-8984a2488ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we first created an RDD, collect_rdd, using the .parallelize() method of SparkContext. Then we used the .collect() method on our RDD which returns the list of all the elements from collect_rdd."
      ],
      "metadata": {
        "id": "tieSwhklpNmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. The .first() Action**\n",
        "\n",
        "The .first() action on an RDD returns the first element from our RDD. This can be helpful when we want to verify if the exact kind of data has been loaded in our RDD as per the requirements. For example, if wanted an RDD with the first 10 natural numbers. We can verify this by checking the first element from our RDD i.e. 1.\n",
        "\n",
        "Here, we first created an RDD, first_rdd using the .parallelize() method of SparkContext having the first ten natural numbers. Then, we applied the .first() operation on first_rdd. This returned the first element from first_rdd, i.e. 1.\n",
        "\n",
        " Let's understand this with an example:"
      ],
      "metadata": {
        "id": "mI7AFAfmqghU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "print(first_rdd.first())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VPc7effuB7H",
        "outputId": "348edffb-c9fb-493d-f7d5-a65b80cfa3b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. The .take() Action**\n",
        "\n",
        "The .take(n) action on an RDD returns n number of elements from the RDD. The ‘n’ argument takes an integer which refers to the number of elements we want to extract from the RDD.\n",
        "\n",
        "Here, we first created an RDD, take_rdd, using the .parallelize() method of SparkContext. Then we applied the .take(3) method on our RDD take_rdd. This returned the first 3 elements in a list from the RDD.\n",
        "\n",
        "Let’s understand this with an example:"
      ],
      "metadata": {
        "id": "fuxOuIGfuntt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "take_rdd = sc.parallelize([1,2,3,4,5])\n",
        "print(take_rdd.take(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p62Z7mkkuyIp",
        "outputId": "bb008e08-c76e-413e-d2bb-f60cbc76da75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. The .reduce() Action**\n",
        "\n",
        "The .reduce() Action takes two elements from the given RDD and operates. This operation is performed using an anonymous function or lambda.\n",
        "\n",
        "Here, we created an RDD, reduce_rdd using .parallelize() method of SparkContext. We used the .reduce action on reduce_rdd with an enclosed anonymous function or lambda. Here, the lambda adds all the elements of the given RDD and prints the sum.\n",
        "\n",
        " For example, if we want to add all the elements from the given RDD, we can use the .reduce() action.\n",
        "\n",
        "**Note: What is lambda function?**"
      ],
      "metadata": {
        "id": "NIkcytKsvask"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_rdd = sc.parallelize([1,3,4,6])\n",
        "print(reduce_rdd.reduce(lambda x, y : x + y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5iPnG9pvdcO",
        "outputId": "9ee37193-8dcb-4207-c701-ee7b8a8abc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. The .saveAsTextFile() Action**\n",
        "\n",
        "The .saveAsTextFile() Action is used to serve the resultant RDD as a text file. We can also specify the path to which file needed to be saved. This helps in saving our results especially when we are working with a large amount of data.\n",
        "\n",
        "\n",
        "Here, we created an RDD, save_rdd using the .parallelize() method of SparkContext. We used the .saveAsTextFile() action on save_rdd to save it into our directory with the name passed as an argument in it as a string type. The .saveAsTextFile() generates a directory with the given argument. Inside the directory, several parts of the file will be created based on the size of the file.\n"
      ],
      "metadata": {
        "id": "UznPEVsbx-Ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_rdd = sc.parallelize([1,2,3,4,5,6])\n",
        "save_rdd.saveAsTextFile('file.txt')"
      ],
      "metadata": {
        "id": "XSLoDlY5ydkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformations in PySpark RDDs**\n",
        "\n",
        "Transformations are the kind of operations that are performed on an RDD and return a new RDD. Few of these methods work almost similarly to the functions already present in Python. To learn more about Transformations, refer to the Spark Documentation here.\n",
        "\n",
        "Now, Let's look at some of the essential Transformations in PySpark RDD:\n",
        "\n",
        "**1. The .map() Transformation**\n",
        "\n",
        "As the name suggests, the .map() transformation maps a value to the elements of an RDD. The .map() transformation takes in an anonymous function and applies this function to each of the elements in the RDD. For example, If we want to add 10 to each of the elements present in RDD, the .map() transformation would come in handy. This operation saves time and goes with the DRY policy.\n",
        "\n",
        "Here, we created an RDD, my_rdd using the .parallelize() method of SparkContext. Since the .map() is a transformation, it returns a new RDD, thus we used the .collect() action to extract all the resultant elements in a Python list. Here, the anonymous function or lambda performs the same as it works in Python.\n",
        "\n",
        "Let's understand this with an example:"
      ],
      "metadata": {
        "id": "bPmSSo847FO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_rdd = sc.parallelize([1,2,3,4])\n",
        "print(my_rdd.map(lambda x: x+ 10).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IeSljXVpPIb",
        "outputId": "4a57af16-5252-490f-f685-61a8815ce6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 12, 13, 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. The .filter() Transformation**\n",
        "\n",
        "A .filter() transformation is an operation in PySpark for filtering elements from a PySpark RDD. The .filter() transformation takes in an anonymous function with a condition. Again, since it’s a transformation, it returns an RDD having elements that had passed the given condition. For example, we want to return only an even number of elements, we can use the .filter() transformation.\n",
        "\n",
        "Here, we first created an RDD, filter_rdd using the .parallelize() method of SparkContext. Then we used the anonymous function lambda to filter the even numbers from our RDD filter_rdd. Since .filter() transformation returns a new RDD, we used the .collect() action to extract all the resultant elements in a list.\n",
        "\n",
        "Let’s understand this with an example:"
      ],
      "metadata": {
        "id": "sxGQZiag7mmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_rdd = sc.parallelize([2, 3, 4, 5, 6, 7])\n",
        "print(filter_rdd.filter(lambda x: x%2 == 0).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gmJw0pKo9G8",
        "outputId": "f87cfb60-ffb6-41ff-e506-485e4c79176f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also filter strings from a certain text present in an RDD. For example, If we want to check the names of persons from a list of guests starting with a certain alphabet, we can use the .filter() for this operation as well.\n",
        "\n",
        "Here, we first created an RDD, filter_rdd_2 using the **.parallelize(**) method of SparkContext. Then we used the .filter() transformation on it to filter the elements of our RDD that start with 'R'. We used the .collect() action on the resultant RDD to get all the desired elements in a list.\n",
        "\n",
        "Let’s understand this with an example:\n",
        "\n"
      ],
      "metadata": {
        "id": "FXpqL3eX8UUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filter_rdd_2 = sc.parallelize(['Rangi', 'Sasma', 'Raneshi', 'Shoba', 'Praide'])\n",
        "print(filter_rdd_2.filter(lambda x: x.startswith('S')).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9tNy5O_8cko",
        "outputId": "26eb9c8c-2dc4-4c76-fd15-c36b077edec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. The .union() Transformation**\n",
        "\n",
        "The .union() transformation combines two RDDs and returns the union of the input two RDDs. This can be helpful to extract elements from similar characteristics from two RDDs into a single RDD.\n",
        "\n",
        "Here, we first created an RDD, union_inp using the .parallelize() method of SparkContext. Then we created two more RDDs union_rdd_1 and union_rdd_2 using the .filter() method on input RDD. At last, we created the union of the two filtered RDDs using the .union() transformation.\n",
        "\n",
        "Let’s understand this with an example:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b9nuSj6f8cWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "union_inp = sc.parallelize([2,4,5,6,7,8,9])\n",
        "union_rdd_1 = union_inp.filter(lambda x: x % 2 == 0)\n",
        "union_rdd_2 = union_inp.filter(lambda x: x % 3 == 0)\n",
        "print(union_rdd_1.union(union_rdd_2).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtgigMMACn0o",
        "outputId": "5e1001d9-bfd9-4eaf-e28b-9c8c9d861a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 6, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. The .flatMap() Transformation**\n",
        "\n",
        "The .flatMap() transformation peforms same as the .map() transformation except the fact that .flatMap() transformation return seperate values for each element from original RDD.\n",
        "\n",
        "Let’s understand this with an example:\n",
        "\n",
        "flatmap_rdd = sc.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
        "(flatmap_rdd.flatMap(lambda x: x.split(\" \")).collect())\n",
        "\n",
        "Here, we first created an RDD, flatmap_rdd using the .parallelize() method and added two strings to it. Then, we applied the .flatMap() transformation to it to split all the strings into single words. This worked the same as the .split() method in Python lists. Then we used the .collect() method to extract all the resultant elements in a list.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HGHhyB0RC01r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flatmap_rdd = sc.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
        "(flatmap_rdd.flatMap(lambda x: x.split(\" \")).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnx12x_rDBes",
        "outputId": "12bb184c-5450-4308-e21a-0de8b2acb315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey', 'there', 'This', 'is', 'PySpark', 'RDD', 'Transformations']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark Pair RDD Operations**\n",
        "\n",
        "PySpark has a dedicated set of operations for Pair RDDs. Pair RDDs are a special kind of data structure in PySpark in the form of key-value pairs, and that’s how it got its name. Practically, the Pair RDDs are used more widely because of the reason that most of the real-world data is in the form of Key/Value pairs. The Pair RDDs use different terminology for key and value. The key is known as the identifier while the value is known as data.\n",
        "\n",
        "Now, let’s see how to create Pair RDDs in PySpark.\n",
        "\n",
        "First, we will create a list of tuples. The following tuples will be having students from a class and their average marks out of 100.\n",
        "\n",
        "marks = [('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78)]\n",
        "sc.parallelize(marks).collect()\n",
        "This returns a PySpark Pair RDD.\n",
        "\n",
        "\n",
        "Now, we will see a set of methods which are the PySpark operations specifically for Pair RDDs. The same set of Actions is perfectly fine for Pair RDDs that had worked for normal RDDs. But Pair RDDs has a unique set of Transformation operations and comes in handy when we have data in key, value pairs."
      ],
      "metadata": {
        "id": "nilTqDzG8b5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marks = [('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78)]\n",
        "sc.parallelize(marks).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkc2EG8oFExI",
        "outputId": "f7457946-9d36-4d83-a42f-932fcce3c831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following are the widely used Transformation on a Pair RDD:\n",
        "\n",
        "**1. The .reduceByKey() Transformation**\n",
        "\n",
        "The .reduceByKey() transformation performs multiple parallel processes for each key in the data and combines the values for the same keys. It uses an anonymous function or lambda to perform the task. Since it's a transformation, it returns an RDD as a result.\n",
        "\n",
        "In the following code,  we created an RDD, marks_rdd using the .parallelize() method of SparkContext and added a list of tuples consisting of marks of students. Then, we applied the .groupByKey() transformation on marks_rdd with an anonymous function enclosing inside the .reduceByKey(). This returns a new RDD and thus we applied the .collect() action to generate the list of resultant elements."
      ],
      "metadata": {
        "id": "YKSn2jhiaj6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
        "print(marks_rdd.reduceByKey(lambda x, y: x + y).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebkFesewa1mN",
        "outputId": "d8e737a7-b569-42d2-b5e3-5e57a9763577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Rahul', 48), ('Shreya', 50), ('Rohan', 44), ('Swati', 45), ('Abhay', 55)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. The .sortByKey() Transformation**\n",
        "\n",
        "The .sortByKey() transformation sorts the input data by keys from key-value pairs either in ascending or descending order. It returns a unique RDD as a result.\n",
        "\n",
        "In the coding below, we created an RDD, marks_rdd using the **.parallelize() **method of SparkContext and added a list of tuples consisting of marks of students. We applied the **.sortByKey()** Transformation on this RDD. We also passed ‘ascending‘ (string) as an argument to the .sortByKey() transformation which denotes that we want to sort the keys in ascending order. At last, we used the .collect() method on the resultant RDD to get all the result elements as a list."
      ],
      "metadata": {
        "id": "IUhH0Q-3bp44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
        "print(marks_rdd.sortByKey('ascending').collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghv25zL5bvv_",
        "outputId": "bfb50db3-eea7-4362-b184-ef4e16f43159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Abhay', 29), ('Abhay', 26), ('Rahul', 25), ('Rahul', 23), ('Rohan', 22), ('Rohan', 22), ('Shreya', 22), ('Shreya', 28), ('Swati', 26), ('Swati', 19)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. The .groupByKey() Transformation**\n",
        "\n",
        "The .groupByKey() transformation groups all the values in the given data with the same key together. It returns a new RDD as a result. For example, if we want to extract all the Cultural Members from a list of committee members, the .groupByKey() will come in handy to perform the necessary task.\n",
        "\n",
        "\n",
        "In code script given below we created an RDD, marks_rdd using the .parallelize() method of SparkContext and added a list of tuples consisting of marks of students. Then, we applied the **.groupByKey()** transformation on the marks_rdd RDD. Then we used the .collect() action to get the results and saved the results to dict_rdd. Since dict_rdd is a dictionary item type, we applied the for loop on dict_rdd  to get a list of marks for each student in each line. We also added list() to the values since we have more than one subject mark for students."
      ],
      "metadata": {
        "id": "xO1ECN6WcUa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
        "dict_rdd = marks_rdd.groupByKey().collect()\n",
        "for key, value in dict_rdd:\n",
        "    print(key, list(value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_X5SYF3d4ga",
        "outputId": "872cb75c-dd1e-4609-b3d3-81d1d1712e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rahul [25, 23]\n",
            "Shreya [22, 28]\n",
            "Rohan [22, 22]\n",
            "Swati [26, 19]\n",
            "Abhay [29, 26]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actions in Pair RDDs**\n",
        "\n",
        "Even though all of the RDD Actions can be performed on Pair RDDs, there is a set of articles that are specifically designed for Pair RDDs. These Actions will not work on normal RDDs and are to be used only on Pair RDDs. Following are the Actions that are widely used for Key-Value type Pair RDD data:\n",
        "\n",
        "**1. The countByKey() Action**\n",
        "The .countByKey() option is used to count the number of values for each key in the given data. This action returns a dictionary and one can extract the keys and values by iterating over the extracted dictionary using loops. Since we are getting a dictionary as a result, we can also use the dictionary methods such as .keys(), .values() and .items()."
      ],
      "metadata": {
        "id": "azqxyrAUb_CX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Rohan', 22), ('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
        "dict_rdd = marks_rdd.countByKey().items()\n",
        "for keyk, value in dict_rdd:\n",
        "    print(keyk, value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy3p_epCfdYq",
        "outputId": "3ebef770-1a08-420c-b595-0a5160c8cb27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rahul 2\n",
            "Swati 2\n",
            "Rohan 2\n",
            "Shreya 1\n",
            "Abhay 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we created an RDD, marks_rdd using the .parallelize() method of SparkContext and added a list of tuples consisting of marks of students. The .countByKey() action returns the dictionary, we saved the dictionary items into variable dict_rdd. Later we iterated over these items and got the count of values for each key."
      ],
      "metadata": {
        "id": "XleDD4UXfzjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1v2bUvHvbpZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fPt76eyMbTOO"
      }
    }
  ]
}